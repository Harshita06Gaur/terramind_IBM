{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hex2color, LinearSegmentedColormap\n",
    "from terratorch import FULL_MODEL_REGISTRY\n",
    "\n",
    "# Select device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'    \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load input data\n",
    "examples = [\n",
    "    '38D_378R_2_3.tif',\n",
    "    '282D_485L_3_3.tif',\n",
    "    '433D_629L_3_1.tif',\n",
    "    '637U_59R_1_3.tif',\n",
    "    '609U_541L_3_0.tif',\n",
    "]\n",
    "\n",
    "# Select example between 0 and 4\n",
    "file = examples[0]\n",
    "\n",
    "# Define modalities\n",
    "modalities = ['S2L2A', 'S1RTC', 'DEM', 'LULC', 'NDVI']\n",
    "data = {m: rxr.open_rasterio(f'../examples/{m}/{file}') for m in modalities}\n",
    "# Tensor with shape [B, C, 224, 224]\n",
    "data = {\n",
    "    k: torch.Tensor(v.values, device='cpu').unsqueeze(0)\n",
    "    for k, v in data.items()\n",
    "}"
   ],
   "id": "43b2c3eb41e450d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run any-to-any generation (this can take a while without a GPU, consider reducing timesteps for faster inference)\n",
    "outputs = {}\n",
    "for m in modalities:\n",
    "    print(f'Processing {m}')\n",
    "    out_modalities = modalities[:]\n",
    "    out_modalities.remove(m)\n",
    "    \n",
    "    # Init model\n",
    "    model = FULL_MODEL_REGISTRY.build(\n",
    "        'terramind_v1_base_generate',\n",
    "        modalities=[m],\n",
    "        output_modalities=out_modalities,\n",
    "        pretrained=True,\n",
    "        standardize=True,\n",
    "    )\n",
    "    _ = model.to(device)\n",
    "    \n",
    "    input = data[m].clone().to(device)\n",
    "    with torch.no_grad():\n",
    "      generated = model(input, verbose=True, timesteps=10)\n",
    "    outputs[m] = generated"
   ],
   "id": "f399d4fb83a5adfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting utils\n",
    "COLORBLIND_HEX = [\"#000000\", \"#3171AD\", \"#469C76\", '#83CA70', \"#EAE159\", \"#C07CB8\", \"#C19368\", \"#6FB2E4\", \"#F1F1F1\", \"#C66526\"]   \n",
    "COLORBLIND_RGB = [hex2color(hex) for hex in COLORBLIND_HEX]\n",
    "esri_cmap = LinearSegmentedColormap.from_list('esri', COLORBLIND_RGB, N=10)\n",
    "\n",
    "def plot_modality_data(modality, data, ax):\n",
    "    # Remove batch dim\n",
    "    data = data[0].clone().cpu().numpy()\n",
    "    interpolation = cmap = vmin = vmax = None\n",
    "    if modality in ['S2L1C', 'S2L2A']:\n",
    "        data = data[[3,2,1]]\n",
    "        data = data.clip(0, 2000)\n",
    "        data = (data / 2000 * 255).astype(np.uint8)\n",
    "    elif modality in ['S1RTC', 'S1GRD']:\n",
    "        # RGB bands [VH, VV, VH]\n",
    "        vv = data[0:1]\n",
    "        vh = data[1:2]\n",
    "        vv = vv.clip(-30, 5)\n",
    "        vv = ((vv + 30) / 35 * 255).astype(np.uint8)\n",
    "        vh = vh.clip(-40, 0)\n",
    "        vh = ((vh + 40) / 40 * 255).astype(np.uint8)\n",
    "        data = np.concatenate([vh, vv, vh], axis=0)\n",
    "    elif modality in ['NDVI']:\n",
    "        cmap = 'RdYlGn'\n",
    "        vmin, vmax = -1, 1\n",
    "    elif modality in ['DEM']:\n",
    "        data_min, data_max = np.min(data) - 5, np.max(data) + 5\n",
    "        data = (data - data_min) / (data_max - data_min + 1e-6) * 255\n",
    "        cmap = 'BrBG_r'\n",
    "        vmin, vmax= 0, 255\n",
    "    elif modality in ['LULC']:\n",
    "        if data.shape[0] > 1:\n",
    "            data = data.argmax(axis=0)\n",
    "        cmap = esri_cmap\n",
    "        vmin, vmax= 0, 9\n",
    "        interpolation = 'nearest'\n",
    "    else:\n",
    "        raise ValueError(f'Unknown modality: {modality}')\n",
    "\n",
    "    if len(data.shape) == 3:\n",
    "        data = data.transpose(1,2,0)\n",
    "    \n",
    "    ax.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax, interpolation=interpolation)\n",
    "    ax.axis('off')"
   ],
   "id": "284e38836986129f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot generations\n",
    "n_mod = len(modalities)\n",
    "fig, axes = plt.subplots(nrows=n_mod, ncols=n_mod + 1, figsize=[12, 10])\n",
    "axes[0][0].set_title('Input')\n",
    "for i, m in enumerate(modalities):\n",
    "    axes[0][i + 1].set_title(m)\n",
    "\n",
    "for (m, input), ax in zip(data.items(), axes):\n",
    "    plot_modality_data(m, input, ax[0])\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "\n",
    "for k, m_output in enumerate(outputs.values()):\n",
    "    for m, out in m_output.items():        \n",
    "        j = modalities.index(m) + 1\n",
    "        plot_modality_data(m, out, axes[k][j])\n",
    "        \n",
    "plt.savefig(f'any_to_any_{os.path.basename(file)}.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Note: TerraMind uses chained generations (see 4M for details)"
   ],
   "id": "9ced943a8b005719",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
